---
title: Toward Cognitive Models in Explainable AI: Mechanistic Interpretability as a Missing Link?

event: Biannual Conference of the European Society for Philosophy of Science
event_url: https://philsci.eu/EPSA23

location: Belgrade, Serbia 
# address:
#  street: 450 Serra Mall
#  city: Stanford
#  region: CA
#  postcode: '94305'
#  country: United States

summary: In this talk, we evaluate the prospects of mechanistic interpretability as a cognitive modeling framework for explainable AI. 
# abstract: 'Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellusac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam.'

# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
date: '2023-09-20'
date_end: '2023-09-23'
all_day: true

# Schedule page publish date (NOT talk date).
publishDate: '2017-01-01T00:00:00Z'

authors: []
tags: []

# Is this a featured talk? (true/false)
featured: false

image:
  # caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/bzdhc5b3Bxs)'
  focal_point: Right

links:
  - icon: twitter
    icon_pack: fab
    name: Follow
    url: https://twitter.com/cebudding
url_code: ''
url_pdf: ''
url_slides: ''
url_video: ''

# Markdown Slides (optional).
#   Associate this talk with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides: ""

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
#  - tacit-knowledge-and-llms


# {{% callout note %}}
# Click on the **Slides** button above to view the built-in slides feature.
# {{% /callout %}}

# Slides can be added in a few ways:

# - **Create** slides using Wowchemy's [_Slides_](https://wowchemy.com/docs/managing-content/#create-slides) feature and link using `slides` parameter in the front matter of the talk file
# - **Upload** an existing slide deck to `static/` and link using `url_slides` parameter in the front matter of the talk file
# - **Embed** your slides (e.g. Google Slides) or presentation video on this page using [shortcodes](https://wowchemy.com/docs/writing-markdown-latex/).

#Further event details, including [page elements](https://wowchemy.com/docs/writing-markdown-latex/) such as image galleries, can be added to the body of this page.
---
Because Explainable AI faces challenges similar to the ones facing cognitive science, the explanatory strategies of the latter may be useful guides for the former. Top-down and bottom-up strategies are used in cognitive science to create cognitive models, which describe cognitive processes as computational algorithms. However, such models remain elusive in XAI. Mechanistic interpretability is an approach that identifies interpretable structure within a network to explain its global behavior. By studying a network's internal parameters, this method determines the algorithm it has learned. This approach has made first steps toward explaining machine vision and natural language processing. Although preliminary, this work resembles cognitive modeling efforts in cognitive science by combining top-down behavioral observations with bottom-up investigations of the underlying mechanisms. Therefore, mechanistic interpretability deserves closer philosophical scrutiny.

Collaborative work with Carlos Zednik. 

# > [Slides can be found here.](https://docs.google.com/presentation/d/e/2PACX-1vTDmugRGLTnQMpRs1cQGpxquL87GznTKcAVvZIXzHfbe88sBdZ4amau3RuYlBywm00cXw46nmpDCzJw/pub?start=false&loop=false&delayms=3000)
